spark.executor.memory=! (node memory - 2g - overhead 10%) / executors per node
spark.executor.instances=! node count * executors per node
spark.executor.cores=! node cores / executors per node

spark.network.timeout=10000s
spark.speculation=false
spark.master=yarn
spark.submit.deployMode=cluster
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2

spark.cores.max=! node cores * node count

spark.driver.cores=dummy
spark.driver.memory=dummy
spark.driver.maxResultSize=100g

maximizeResourceAllocation=false

# if more than 100 config lines about to go here, use tasks.ini instead

spark.executor.memory=! (node memory - 2g - overhead 10%) / executors per node
spark.executor.instances=! node count * executors per node
spark.cores.max=! node cores * node count
spark.executor.cores=! node cores / executors per node

spark.network.timeout=1000
spark.speculation=false
spark.master=yarn-cluster
spark.driver.maxResultSize=3g

maximizeResourceAllocation={true|false} if true, all resources on nodes will be utilized by executors, but one node will run exclusively a driver, with no regular executors at all
